\section{Algorithmus}

Bei der Betrachtung kleinerer Problemgrößen fällt bei der Wahl eines Algorithmus weniger seine Effizienz ins Gewicht, sondern eher die Einfachheit seiner Implementierung und seine Verständlichkeit (vgl. \cite[5 f.]{GD18a}).\\

\noindent
So läuft bspw. eine Implementierung von Insertion-Sort ($O(n^2)$), die zur Sortierung $8*n^2$ Operationen benötigt, für Eingabemengen $n \leq 43$ schneller als eine Implementierung von Merge-Sort ($O(n\ log(n))$), die $64 * (n\ log(n))$ Schritte für die Sortierung benötigt.\\

\noindent
Diesbzgl. hatte der Selbsttest ``D+A-Selbsttest-02: O-Notation`` die Frage gestellt, in welchem Fall die Beurteilung eines Algorithmus bezüglich der Komplexitätsklasse nicht unbedingt angemessen sei. Die richtige Antwort hierzu lautete: ``Bei sehr kleinen Eingabemengen``.\\

\subsection{Optimaler Algorithmus}
\begin{tcolorbox}[title={Optimaler Algorithmus}]
    ``Ein Algorithmus heißt (asymptotisch) \textbf{optimal}, wenn die obere Schranke für seine Laufzeit mit der unteren Schranke für die Komplexität des Problems zusammenfällt.`` (~\cite[20]{GD18a})\\

    \noindent
    Sortieralgorithmen mit einer Laufzeit von $O(n\ log\ n)$ sind \textbf{optimal}, bspw. \textbf{Merge-Sort}.
\end{tcolorbox}


\subsection{Divide-and-conquer (DAC)}

\textit{Ottmann und Widmayer} formulieren \textbf{DAC} wie folgt (vgl.~\cite[9]{OW17a}):

\begin{tcolorbox}[title={Divide-and-conquer-Verfahren zur Lösung eines Problems der Größe N}]
   \begin{enumerate}
       \item \textit{Divide}: Teile das Problem der Größe $N$ in (wenigstens) zwei annähernd gleich große Teilprobleme, wenn $N > 1$ ist; sonst löse das Problem der Größe $1$ direkt
       \item \textit{Conquer}: Löse die Teilprobleme auf dieselbe Art (rekursiv).
       \item \textit{Merge}: Füge die Teillösungen zur Gesamtlösung zusammen.
   \end{enumerate}
\end{tcolorbox}
